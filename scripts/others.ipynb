{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Apa itu ROI?\n",
      "A: rasio keuntungan atau kerugian yang dihasilkan dari investasi relatif terhadap biaya investasi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TFBertForQuestionAnswering, BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/final_dataset.csv')\n",
    "\n",
    "# Define a function to find context based on the user's question\n",
    "def find_context_for_question(question, dataset):\n",
    "    for _, row in dataset.iterrows():\n",
    "        if row['question'].strip().lower() == question.strip().lower():\n",
    "            return row['context']\n",
    "    return None\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"Rifky/Indobert-QA\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = TFBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Function to answer a question using the model and found context\n",
    "def answer_question(question):\n",
    "    context = find_context_for_question(question, df)\n",
    "    if context is None:\n",
    "        return \"Pertanyaan tidak ditemukan dalam dataset.\"\n",
    "    \n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    answer_start = tf.argmax(outputs.start_logits, axis=1).numpy()[0]\n",
    "    answer_end = tf.argmax(outputs.end_logits, axis=1).numpy()[0] + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "user_question = \"Apa itu ROI?\"\n",
    "answer = answer_question(user_question)\n",
    "print(f\"Q: {user_question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9597133994102478, 'start': 176, 'end': 190, 'answer': '8 Januari 1855'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inisialisasi pipeline untuk question answering\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"Rifky/Indobert-QA\",\n",
    "    tokenizer=\"Rifky/Indobert-QA\"\n",
    ")\n",
    "\n",
    "# Contoh penggunaan pipeline\n",
    "context = \"\"\"\n",
    "Pangeran Harya Dipanegara (atau biasa dikenal dengan nama Pangeran Diponegoro, \n",
    "lahir di Ngayogyakarta Hadiningrat, 11 November 1785 â€“ meninggal di Makassar, \n",
    "Hindia Belanda, 8 Januari 1855 pada umur 69 tahun) adalah salah seorang pahlawan \n",
    "nasional Republik Indonesia, yang memimpin Perang Diponegoro atau Perang Jawa selama \n",
    "periode tahun 1825 hingga 1830 melawan pemerintah Hindia Belanda. Sejarah mencatat, \n",
    "Perang Diponegoro atau Perang Jawa dikenal sebagai perang yang menelan korban terbanyak \n",
    "dalam sejarah Indonesia, yakni 8.000 korban serdadu Hindia Belanda, 7.000 pribumi, \n",
    "dan 200 ribu orang Jawa serta kerugian materi 25 juta Gulden.\n",
    "\"\"\"\n",
    "question = \"kapan pangeran diponegoro meninggal?\"\n",
    "\n",
    "# Melakukan penjawaban pertanyaan\n",
    "result = qa_pipeline({\n",
    "    'context': context,\n",
    "    'question': question\n",
    "})\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02114c5c65e34bdbbbf33c0421d3bc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alifs\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d170cdfbf17432ca142c4f6005e785f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f305ad04b364429a76fe06bca32021c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00faa19013e14be49b24270c0bc1504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860f6d3e54334a9da0f4db1a1b74c2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fec202dca374467bcf9e04f4a65de31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9066bc61e94f6ab7d89fa9e363333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hari ini cuacanya sangat cerah dan saya merasa.\n",
      "\n",
      "\"I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'openai-community/gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"My name is Merve and my favorite\"\n",
    "\n",
    "# Encode the prompt into tokens\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text back into string\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
