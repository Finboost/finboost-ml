{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Apa itu ROI?\n",
      "A: rasio keuntungan atau kerugian yang dihasilkan dari investasi relatif terhadap biaya investasi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TFBertForQuestionAnswering, BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/final_dataset.csv')\n",
    "\n",
    "# Define a function to find context based on the user's question\n",
    "def find_context_for_question(question, dataset):\n",
    "    for _, row in dataset.iterrows():\n",
    "        if row['question'].strip().lower() == question.strip().lower():\n",
    "            return row['context']\n",
    "    return None\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"Rifky/Indobert-QA\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = TFBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Function to answer a question using the model and found context\n",
    "def answer_question(question):\n",
    "    context = find_context_for_question(question, df)\n",
    "    if context is None:\n",
    "        return \"Pertanyaan tidak ditemukan dalam dataset.\"\n",
    "    \n",
    "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    answer_start = tf.argmax(outputs.start_logits, axis=1).numpy()[0]\n",
    "    answer_end = tf.argmax(outputs.end_logits, axis=1).numpy()[0] + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "user_question = \"Apa itu ROI?\"\n",
    "answer = answer_question(user_question)\n",
    "print(f\"Q: {user_question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9597133994102478, 'start': 176, 'end': 190, 'answer': '8 Januari 1855'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inisialisasi pipeline untuk question answering\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"Rifky/Indobert-QA\",\n",
    "    tokenizer=\"Rifky/Indobert-QA\"\n",
    ")\n",
    "\n",
    "# Contoh penggunaan pipeline\n",
    "context = \"\"\"\n",
    "Pangeran Harya Dipanegara (atau biasa dikenal dengan nama Pangeran Diponegoro, \n",
    "lahir di Ngayogyakarta Hadiningrat, 11 November 1785 â€“ meninggal di Makassar, \n",
    "Hindia Belanda, 8 Januari 1855 pada umur 69 tahun) adalah salah seorang pahlawan \n",
    "nasional Republik Indonesia, yang memimpin Perang Diponegoro atau Perang Jawa selama \n",
    "periode tahun 1825 hingga 1830 melawan pemerintah Hindia Belanda. Sejarah mencatat, \n",
    "Perang Diponegoro atau Perang Jawa dikenal sebagai perang yang menelan korban terbanyak \n",
    "dalam sejarah Indonesia, yakni 8.000 korban serdadu Hindia Belanda, 7.000 pribumi, \n",
    "dan 200 ribu orang Jawa serta kerugian materi 25 juta Gulden.\n",
    "\"\"\"\n",
    "question = \"kapan pangeran diponegoro meninggal?\"\n",
    "\n",
    "# Melakukan penjawaban pertanyaan\n",
    "result = qa_pipeline({\n",
    "    'context': context,\n",
    "    'question': question\n",
    "})\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02114c5c65e34bdbbbf33c0421d3bc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alifs\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d170cdfbf17432ca142c4f6005e785f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f305ad04b364429a76fe06bca32021c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00faa19013e14be49b24270c0bc1504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860f6d3e54334a9da0f4db1a1b74c2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fec202dca374467bcf9e04f4a65de31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9066bc61e94f6ab7d89fa9e363333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hari ini cuacanya sangat cerah dan saya merasa.\n",
      "\n",
      "\"I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am a woman who has been married for over a year. I am\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'openai-community/gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"My name is Merve and my favorite\"\n",
    "\n",
    "# Encode the prompt into tokens\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text back into string\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alifs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a325c9e64bd451c94a8776c8a0e6062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alifs\\.cache\\huggingface\\hub\\models--indobenchmark--indobert-large-p2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b4cad2004b43a6ba0c85bc3ad1ec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7597b55214436890aec5f28b43dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400bc22d38c3441c922de5c394bc857f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5f6671af2846a5a1d8481c144904dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'id_cord19' doesn't exist on the Hub or cannot be accessed. If the dataset is private or gated, make sure to log in with `huggingface-cli login` or visit the dataset page at https://huggingface.co/datasets/id_cord19 to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_cord19\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# An Indonesian financial QA dataset\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m     77\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\datasets\\load.py:2587\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2582\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2583\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2584\u001b[0m )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2587\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2588\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2589\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2590\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2591\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2592\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2593\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2594\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2595\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2596\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2597\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2598\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2599\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2600\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2601\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2602\u001b[0m )\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\datasets\\load.py:2259\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2257\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2258\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2259\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\datasets\\load.py:1904\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[1;32m-> 1904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1907\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1909\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alifs\\anaconda3\\envs\\main-ds\\lib\\site-packages\\datasets\\load.py:1850\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1849\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at revision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;28;01melse\u001b[39;00m msg\n\u001b[1;32m-> 1850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[0;32m   1851\u001b[0m         msg\n\u001b[0;32m   1852\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the dataset is private or gated, make sure to log in with `huggingface-cli login` or visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ask for access.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1853\u001b[0m     )\n\u001b[0;32m   1854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: Dataset 'id_cord19' doesn't exist on the Hub or cannot be accessed. If the dataset is private or gated, make sure to log in with `huggingface-cli login` or visit the dataset page at https://huggingface.co/datasets/id_cord19 to ask for access."
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from transformers import BertTokenizerFast, TFBertForQuestionAnswering, create_optimizer, DefaultDataCollator\n",
    "# from datasets import load_dataset, Dataset\n",
    "# from transformers import TrainingArguments\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import f1_score\n",
    "# import nltk\n",
    "# import re\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Define model and tokenizer\n",
    "# model_name = \"indobenchmark/indobert-large-p2\"  # An advanced model for Indonesian language\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "# model = TFBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Text cleaning function\n",
    "# def clean_text(text):\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "#     return text.strip()\n",
    "\n",
    "# # Tokenize dataset\n",
    "# def preprocess_function(examples):\n",
    "#     questions = [clean_text(q) for q in examples[\"question\"]]\n",
    "#     contexts = [clean_text(c) for c in examples[\"context\"]]\n",
    "#     inputs = tokenizer(\n",
    "#         questions,\n",
    "#         contexts,\n",
    "#         max_length=256,\n",
    "#         truncation=\"only_second\",\n",
    "#         return_offsets_mapping=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "#     answers = examples[\"answer\"]\n",
    "#     start_positions = []\n",
    "#     end_positions = []\n",
    "\n",
    "#     for i, (answer, offset) in enumerate(zip(answers, offset_mapping)):\n",
    "#         start_char = examples[\"answer_start\"][i]\n",
    "#         end_char = start_char + len(answer)\n",
    "\n",
    "#         sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "#         # Find the start and end of the context\n",
    "#         idx = 0\n",
    "#         while sequence_ids[idx] != 1:\n",
    "#             idx += 1\n",
    "#         context_start = idx\n",
    "#         while sequence_ids[idx] == 1:\n",
    "#             idx += 1\n",
    "#         context_end = idx - 1\n",
    "\n",
    "#         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "#             start_positions.append(0)\n",
    "#             end_positions.append(0)\n",
    "#         else:\n",
    "#             start_idx = context_start\n",
    "#             while start_idx <= context_end and offset[start_idx][0] <= start_char:\n",
    "#                 start_idx += 1\n",
    "#             start_positions.append(start_idx - 1)\n",
    "\n",
    "#             end_idx = context_start\n",
    "#             while end_idx <= context_end and offset[end_idx][1] < end_char:\n",
    "#                 end_idx += 1\n",
    "#             end_positions.append(end_idx - 1)\n",
    "\n",
    "#     inputs[\"start_positions\"] = start_positions\n",
    "#     inputs[\"end_positions\"] = end_positions\n",
    "#     return inputs\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset('id_cord19', 'QA')  # An Indonesian financial QA dataset\n",
    "\n",
    "# # Apply preprocessing\n",
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# # Split the tokenized dataset into train and validation sets\n",
    "# train_test_split = tokenized_datasets['train'].train_test_split(test_size=0.1)\n",
    "# train_dataset = train_test_split['train']\n",
    "# val_dataset = train_test_split['test']\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"../models/fine_tuned_model\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='../logs',\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# def create_tf_dataset(tokenized_datasets):\n",
    "#     def generator():\n",
    "#         for example in tokenized_datasets:\n",
    "#             yield (\n",
    "#                 {\n",
    "#                     'input_ids': example['input_ids'],\n",
    "#                     'attention_mask': example['attention_mask']\n",
    "#                 },\n",
    "#                 (\n",
    "#                     example['start_positions'],\n",
    "#                     example['end_positions']\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         generator,\n",
    "#         output_signature=(\n",
    "#             {\n",
    "#                 'input_ids': tf.TensorSpec(shape=(256,), dtype=tf.int32),\n",
    "#                 'attention_mask': tf.TensorSpec(shape=(256,), dtype=tf.int32)\n",
    "#             },\n",
    "#             (\n",
    "#                 tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "#                 tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "#             )\n",
    "#         )\n",
    "#     ).batch(4)\n",
    "\n",
    "# train_tf_dataset = create_tf_dataset(train_dataset)\n",
    "# val_tf_dataset = create_tf_dataset(val_dataset)\n",
    "\n",
    "# # Loss and accuracy functions\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     y_true_start, y_true_end = y_true\n",
    "#     y_pred_start, y_pred_end = y_pred\n",
    "\n",
    "#     start_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_start, y_pred_start, from_logits=True)\n",
    "#     end_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_end, y_pred_end, from_logits=True)\n",
    "    \n",
    "#     return (start_loss + end_loss) / 2\n",
    "\n",
    "# def compute_start_logits_accuracy(y_true, y_pred):\n",
    "#     return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# def compute_end_logits_accuracy(y_true, y_pred):\n",
    "#     return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# # Create optimizer and compile model\n",
    "# num_train_steps = len(train_tf_dataset) * training_args.num_train_epochs\n",
    "# optimizer, lr_schedule = create_optimizer(\n",
    "#     init_lr=training_args.learning_rate,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_train_steps=num_train_steps,\n",
    "#     weight_decay_rate=training_args.weight_decay,\n",
    "# )\n",
    "\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss=custom_loss,\n",
    "#               metrics={\n",
    "#                   'start_positions': compute_start_logits_accuracy,\n",
    "#                   'end_positions': compute_end_logits_accuracy\n",
    "#               })\n",
    "\n",
    "# # Callback to log and plot metrics\n",
    "# class LossAccuracyF1Logger(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, validation_data):\n",
    "#         super(LossAccuracyF1Logger, self).__init__()\n",
    "#         self.validation_data = validation_data\n",
    "#         self.epoch_loss = []\n",
    "#         self.epoch_start_accuracy = []\n",
    "#         self.epoch_end_accuracy = []\n",
    "#         self.epoch_f1 = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         self.epoch_loss.append(logs['loss'])\n",
    "#         self.epoch_start_accuracy.append(logs['start_positions_sparse_categorical_accuracy'])\n",
    "#         self.epoch_end_accuracy.append(logs['end_positions_sparse_categorical_accuracy'])\n",
    "\n",
    "#         # Calculate F1 score on validation data\n",
    "#         predictions_val, true_labels_val = self._predict(self.validation_data)\n",
    "#         f1_val = self.compute_f1_score(predictions_val, true_labels_val)\n",
    "#         self.epoch_f1.append(f1_val)\n",
    "\n",
    "#         # Plot the metrics\n",
    "#         self.plot()\n",
    "\n",
    "#     def _predict(self, dataset):\n",
    "#         predictions = []\n",
    "#         true_labels = []\n",
    "#         for batch in dataset:\n",
    "#             inputs = {'input_ids': batch[0]['input_ids'], 'attention_mask': batch[0]['attention_mask']}\n",
    "#             true_labels.extend(batch[1][0].numpy())  # start_positions\n",
    "#             true_labels.extend(batch[1][1].numpy())  # end_positions\n",
    "#             start_logits, end_logits = model.predict(inputs)\n",
    "#             pred_start = tf.argmax(start_logits, axis=-1).numpy()\n",
    "#             pred_end = tf.argmax(end_logits, axis=-1).numpy()\n",
    "#             predictions.extend(pred_start)\n",
    "#             predictions.extend(pred_end)\n",
    "#         return predictions, true_labels\n",
    "\n",
    "#     def compute_f1_score(self, predictions, true_labels):\n",
    "#         # Calculate F1 score\n",
    "#         f1_val = f1_score(true_labels, predictions, average='weighted')\n",
    "#         return f1_val\n",
    "\n",
    "#     def plot(self):\n",
    "#         # Plot the metrics\n",
    "#         plt.figure(figsize=(18, 5))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.plot(self.epoch_loss, label='Loss')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend()\n",
    "#         plt.title('Training Loss')\n",
    "\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.plot(self.epoch_start_accuracy, label='Start Accuracy')\n",
    "#         plt.plot(self.epoch_end_accuracy, label='End Accuracy')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('Accuracy')\n",
    "#         plt.legend()\n",
    "#         plt.title('Training Accuracy')\n",
    "\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.plot(range(1, len(self.epoch_f1) + 1), self.epoch_f1, label='F1 Score')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('F1 Score')\n",
    "#         plt.legend()\n",
    "#         plt.title('Validation F1 Score')\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "# logger = LossAccuracyF1Logger(validation_data=val_tf_dataset)\n",
    "\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# try:\n",
    "#     model.fit(train_tf_dataset, epochs=training_args.num_train_epochs, callbacks=[logger])\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during training: {e}\")\n",
    "#     raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
